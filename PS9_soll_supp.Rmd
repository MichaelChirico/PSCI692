---
title: "Supplement to PS 9 Solutions"
author: "Michael Chirico"
date: "Compiled `r format(Sys.time(), '%B %d, %Y at %R')`"
output: 
  rmarkdown::html_document
---

# Problem 1

## (d) 
```{r oneD}
library(data.table)

thermo = fread("q1data.csv")
#Preview data
thermo

#find mean & standard deviation
thermo[ , .(mean = mean(x), st.dev = sd(x))]
```

## (e)

R makes it really easy to calculate both more-exact t-distributed confidence intervals and the approximate intervals generated from the corresponding normal distribution:

```{r oneE1}
#Exact CI, 90%
thermo[ , .(lower = mean(x) - qt(.95, df = .N - 1L)*sd(x)/sqrt(.N),
            upper = mean(x) + qt(.95, df = .N - 1L)*sd(x)/sqrt(.N))]
#Approximate CI, 90% (ever-so-slightly narrower)
thermo[ , .(lower = mean(x) - qnorm(.95)*sd(x)/sqrt(.N),
            upper = mean(x) + qnorm(.95)*sd(x)/sqrt(.N))]
#Now for 95%
thermo[ , .(lower = mean(x) - qt(.975, df = .N - 1L)*sd(x)/sqrt(.N),
            upper = mean(x) + qt(.975, df = .N - 1L)*sd(x)/sqrt(.N))]
thermo[ , .(lower = mean(x) - qnorm(.975)*sd(x)/sqrt(.N),
            upper = mean(x) + qnorm(.975)*sd(x)/sqrt(.N))]
#Now for 99%
thermo[ , .(lower = mean(x) - qt(.995, df = .N - 1L)*sd(x)/sqrt(.N),
            upper = mean(x) + qt(.995, df = .N - 1L)*sd(x)/sqrt(.N))]
thermo[ , .(lower = mean(x) - qnorm(.995)*sd(x)/sqrt(.N),
            upper = mean(x) + qnorm(.995)*sd(x)/sqrt(.N))]
```

We could also use the handy `t.test` function to do this for us all at once. We use `t.test` to test the hypothesis that the mean of `x` is the sample mean of `x`; as a byproduct, it gives us the appropriate confidence intervals. Note that this approach uses the t-distributed critical values by default. `t.test` can also handle an alpha level of 1 for 100% confidence intervals.

```{r oneE2}
sapply(c(.9, .95, .99, 1), function(conf) 
  thermo[ , t.test(x, mu = mean(x), conf.level = conf)$conf.int])
```

## (f)

```{r oneF}
sapply(c(.9, .95, .99), function(conf)
  thermo[ , mean(x) + c(-1, 1)*qnorm((conf+1)/2) * 12/sqrt(.N)])
```

# 2

#(e) Bonus Q

We know that the coverage probability for a given $a$ and $b$ is

$$ p(a, b) = \frac1{a^n} - \frac1{b^n} $$

Recalling the goal of efficiency is to have as small an interval as possible in mind, and remembering that $\hat{\theta}_n$ is itself biased, it makes sense to wonder how to choose $a$ and $b$ so as to minimize the interval width. It's not immediately obvious that $a^{*}$ should be 1 -- it could _ex ante_ be anything between $1$ and $\frac{n+1}{n}$ (the bias adjustment constant found in PS6).

To narrow it down -- we must state the precise problem we have in mind, namely:

$$ \min_{a, b} \left\{ b - a \right\} $$

subject to $1 \leq a < b$ and $\frac1{a^n} - \frac1{b^n} = p$ -- in words, we minimize the distance between $b$ and $a$ given that they give the desired coverage probability.

Noting lots of exponentiation afoot, it makes sense to consider an equivalent version of the problem, namely:

$$ \min_{\alpha, \beta} \left\{ \beta - \alpha \right\} $$

subject to $0 \leq \alpha < \beta$ and $e^{-n\alpha} - e^{-n\beta} = p$ (we've simply replaced $a$ by $e^\alpha$ and $b$ by $e^\beta$).

Solving the coverage constraint for $\beta$ in terms of $\alpha$ we get

$$ \beta(\alpha) = -\frac1{n} \ln \left( e^{-n\alpha} - p \right) $$

Note that the domain of $\beta(\alpha)$ is $[0, -\frac1{n} \ln p)$. In order for the first-order condition of our minimization problem to hold, we must have $\frac{\partial \beta}{\partial \alpha} = 1$ for some $\alpha$ in that domain.

But $\frac{\partial \beta}{\partial \alpha} = \frac1{1 - pe^{n \alpha}} \geq \frac1{1 - p}$ for $\alpha$ in this range.

That means $\beta(\alpha) - \alpha$ is strictly increasing in $\alpha$, so the optimal $\alpha$ is indeed 0.

That means $a^{*} = 1$ and $b^{*} = \frac1{\sqrt[n]{1 - p}}$, the latter of which approaches 1 as $n$ increases. Thus (recalling that this specification is fully flexible given that $\theta_0 \geq \hat{\theta}_n$) the optimal confidence interval takes the form:

$$ \left[ \hat{\theta}_n, \frac1{\sqrt[n]{1 - p}} \hat{\theta}_n \right] $$

Recall that the unbiased estimate for $\theta_0$ is $\frac{n+1}{n} \hat{\theta}_n$. Unlike for the symmetric confidince interval, there's no guarantee that this optimal interval contains this unbiased estimate for all $p$! This is easy to see if we set $p = 0$, in which case the interval collapses to the single point $\hat{\theta}_n$. 

For $\frac{n+1}{n} \hat{\theta}_n$ to be in the interval above, we must have $\frac1{\sqrt[n]{1-p}} \geq \frac{n+1}{n}$, which is in turn equivalent to $p \geq 1 - \left( 1 + \frac1{n} \right)^{-n}$; for $n = 1$, this is $p \geq \frac12$, and as $n \rightarrow \infty$, this increases to the limiting value $p \geq 1 - \frac1{e} \approx .63$.

(So in fact, for any reasonable $n$ and any common $p$ such as $.9$ or $.95$, the interval does in fact contain the unbiased estimate)

# 3

## (a)

Can do this with R (a bit overkill); note that `t.test` gives all all the output we could want -- the $t$ statistic, the $p$-value, and the 95% confidence interval (and we could specify `conf.level` if we wanted a different coverage probability):

```{r threeA}
ben = c(6.1, 3.7, 4.2, 5.2, 5.8)
t.test(ben, alternative = "greater", mu = 4)
```

## (b) & (c)

We'll use `t.test` again. `t.test` will also answer part (c) for us.

```{r threeB}
ben = fread("ben_races.csv")
#preview data
head(ben)

boxplot(ben, horizontal = TRUE,
        names = c("Ours", "Bronze", "Bench"),
        main = "Distribution of Race Times", 
        col = c("red", "blue", "darkgreen"), 
        #the notch gives a confidence interval
        #  around the sample median
        notch = TRUE)

# (i)
ben[ , t.test(ben_on_the_bench, bronze_ben)]
# (ii)
ben[ , t.test(bronze_ben, our_ben)]
# (iii) (also c) i))
ben[ , t.test(ben_on_the_bench, our_ben)]
```

## (d)

Though the $p$-values alone don't tell us the answer, we _can_ test this hypothesis:

```{r threeD}
ben[ , t.test(our_ben - bronze_ben, our_ben - ben_on_the_bench)]
```

So in fact we can't say whether this mean is significantly different. 
